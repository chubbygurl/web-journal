<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Document</title>
    <link rel="stylesheet" href="styles.css" />
  </head>
  <header>
    <h1>Applications Development & Emerging Technologies</h1>
  
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">


<div class="navbar">
  <a href="index.html">Home</a>
  <div class="subnav">
    <button class="subnavbtn">About <i class="fa fa-caret-down"></i></button>
    <div class="subnav-content">
      <a href="Artificial Intelligence.html">Artificial Intelligence</a>
      <a href="Internet of Things.html">Internet of Things</a>
      <a href="Augmented Reality.html">Augmented Reality</a>
      <a href="Data Science.html">Data Science</a>
      <a href="Cloud.html">Cloud & Quantum Computing, Smart Farming, and Embedded System</a>
      <a href="Computer Vision.html">Computer Vision, Cyber Security and Addictive Manufacturing</a>
      <a href="Nanotechnology.html">Nanotechnology</a>
      

      

    </div>
  </div>
</div>
  </header>
  <body>
    <div id="main-content">
      <h1>DATA SCIENCE</h1>
      <section>
        <h2>What is Big Data?</h2>
        <p class="inline">
          The quantities, characters, or symbols on which operations are
          performed by a computer, which may be stored and transmitted in the
          form of electrical signals and recorded on magnetic, optical, or
          mechanical recording media.
        </p>
        <h2>Structure of Big Data</h2>
        <ul>
          <li>
            <b>Volume:</b> The first component of big data is volume, which
            refers to the sheer amount of data that is being generated and
            collected. This can include data from a wide variety of sources,
            such as social media, sensor networks, and transactional data.
          </li>
          <li>
            <b>Velocity:</b> The second component of big data is velocity, which
            refers to the speed at which data is being generated and collected.
            In some cases, data may need to be processed and analyzed in
            real-time, such as with financial trading systems or social media
            platforms.
          </li>
          <li>
            <b>Variety:</b> The third component of big data is variety, which
            refers to the different types of data that are being collected. This
            can include structured data, such as numerical data or data that
            fits into a predefined schema, as well as unstructured data, such as
            text, images, and video.
          </li>
        </ul>
        <h2>Types of Data Analytics</h2>
        <ul>
          <li>
            <b>Descriptive Analytics:</b> This type of analytics describes what
            happened in the past. It is used to summarize and understand
            historical data and identify patterns or trends. Examples include
            sales reports, financial statements, and customer demographics.
          </li>
          <li>
            <b>Diagnostic Analytics:</b> This type of analytics goes beyond
            descriptive analytics by answering why something happened in the
            past. It involves deeper analysis of data to uncover the root causes
            of certain outcomes or trends. Examples include customer
            satisfaction surveys and quality control reports.
          </li>
          <li>
            <b>Predictive Analytics:</b> This type of analytics involves using
            historical data and statistical models to predict future outcomes or
            trends. Predictive analytics can be used to forecast sales, identify
            customer behavior patterns, and predict equipment failures.
          </li>
          <li>
            <b>Prescriptive Analytics:</b> This type of analytics goes beyond
            predictive analytics by suggesting the best course of action to take
            based on predicted outcomes. It involves using optimization
            algorithms and simulation techniques to identify the best possible
            solutions to a particular problem. Examples include supply chain
            optimization and risk management strategies.
          </li>
        </ul>
        <h2>Clustered Computing</h2>
        <p class="inline">
          Clustered computing, also known as cluster computing, is a type of
          computing system in which multiple computers work together to solve
          complex problems or perform large-scale computations.
        </p>
        <p class="inline">
          In a clustered computing environment, each computer is referred to as
          a node, and these nodes are connected to one another through a
          high-speed network. The nodes work together to perform computations,
          with each node contributing its own processing power and memory
          resources to the overall computation.
        </p>
        <p class="inline">
          There are different types of clustered computing systems, including
          high-performance computing (HPC) clusters and load-balancing clusters.
          HPC clusters are used for scientific and engineering applications that
          require large amounts of computing power, such as climate modeling or
          genetic sequencing. Load-balancing clusters are used for applications
          that require high availability and scalability, such as web servers or
          database servers.
        </p>
        <h2>HADOOP ECOSYSTEM</h2>
        <p class="inline">
          Hadoop is a popular open-source framework used for distributed storage
          and processing of large datasets across clusters of computers. The
          Hadoop ecosystem consists of various tools and technologies that work
          together to enable big data processing and analysis.
        </p>
        <h2>How HADOOP Works?</h2>
        <ul>
          <li>
            Data is stored in HDFS: Large datasets are stored across multiple
            nodes in a Hadoop cluster using the Hadoop Distributed File System
            (HDFS). HDFS breaks up the data into smaller pieces, or blocks, and
            replicates them across different nodes for fault tolerance.
          </li>
          <li>
            Data processing with MapReduce: Hadoop uses the MapReduce
            programming model for distributed processing of large datasets.
            MapReduce divides a data processing task into smaller sub-tasks,
            known as mappers and reducers, which can be executed on individual
            nodes in the cluster. The mapper task takes input data and
            transforms it into key-value pairs, while the reducer task takes the
            output from the mapper and aggregates the data.
          </li>
          <li>
            Resource management with YARN: The Yet Another Resource Negotiator
            (YARN) is a resource management framework in Hadoop that schedules
            and manages the resources in the cluster. YARN allows multiple data
            processing engines, such as Spark and HBase, to run simultaneously
            on the same cluster.
          </li>
          <li>
            Data analysis with tools like Hive and Pig: Hadoop provides various
            tools and technologies for querying and analyzing large datasets
            stored in HDFS. For example, Apache Hive is a data warehousing and
            SQL-like query language used for querying and analyzing data stored
            in Hadoop. Apache Pig is a high-level scripting language used for
            data processing and analysis.
          </li>
        </ul>
      </section>
    </div>
  </body>
   
    
  </body>
</html>